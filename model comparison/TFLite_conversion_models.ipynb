{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyO6lcWbQ30QO1Vi8zrQ+N51"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ucxq-YGK3jRl","executionInfo":{"status":"ok","timestamp":1755179846527,"user_tz":-60,"elapsed":30639,"user":{"displayName":"Abul Mohsin","userId":"04862932652821291928"}},"outputId":"d38a6edc-da55-48aa-efad-aa8f680ff266"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4d43y303Gu7","executionInfo":{"status":"ok","timestamp":1755181057936,"user_tz":-60,"elapsed":42241,"user":{"displayName":"Abul Mohsin","userId":"04862932652821291928"}},"outputId":"4e9dc73e-71b9-429a-fb74-ae42a39d6140"},"outputs":[{"output_type":"stream","name":"stdout","text":["TF: 2.19.0\n","GPU hidden (CPU-only).\n","\n","=== Converting: LSTM ===\n","Sources: {'savedmodel': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LSTM_Model_48_12', 'keras': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LSTM_Model_48_12.keras', 'h5': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LSTM_Model_48_12.h5', 'weights': '/content/drive/MyDrive/EdgeMeter_AIv2/data/lstm_48_12_trained.weights.h5'}\n","Saved artifact at '/tmp/tmpp8jhcdt0'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 48, 11), dtype=tf.float32, name='keras_tensor_24')\n","Output Type:\n","  TensorSpec(shape=(1, 12), dtype=tf.float32, name=None)\n","Captures:\n","  135312549787984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549788752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549786832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549789328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549786064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LSTM/LSTM_fp32.tflite  (0.4558 MB)\n","Saved artifact at '/tmp/tmpsaaicl3r'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 48, 11), dtype=tf.float32, name='keras_tensor_24')\n","Output Type:\n","  TensorSpec(shape=(1, 12), dtype=tf.float32, name=None)\n","Captures:\n","  135312549787984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549788752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549786832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549789328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549786064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LSTM/LSTM_fp16.tflite  (0.3176 MB)\n","Saved artifact at '/tmp/tmpkjba_851'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 48, 11), dtype=tf.float32, name='keras_tensor_24')\n","Output Type:\n","  TensorSpec(shape=(1, 12), dtype=tf.float32, name=None)\n","Captures:\n","  135312549787984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549788752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549786832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549789328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549786064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LSTM/LSTM_int8_dynamic.tflite  (0.2613 MB)\n","Saved artifact at '/tmp/tmp_39u6nsl'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 48, 11), dtype=tf.float32, name='keras_tensor_24')\n","Output Type:\n","  TensorSpec(shape=(1, 12), dtype=tf.float32, name=None)\n","Captures:\n","  135312549787984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549788752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549786832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549789328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  135312549786064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LSTM/LSTM_int8_full.tflite  (0.6962 MB)\n","{\n","  \"model\": \"LSTM\",\n","  \"artifacts\": [\n","    {\n","      \"variant\": \"fp32\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LSTM/LSTM_fp32.tflite\",\n","      \"size_mb\": 0.4558,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"fp16\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LSTM/LSTM_fp16.tflite\",\n","      \"size_mb\": 0.3176,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"int8_dynamic\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LSTM/LSTM_int8_dynamic.tflite\",\n","      \"size_mb\": 0.2613,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"int8_full\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LSTM/LSTM_int8_full.tflite\",\n","      \"size_mb\": 0.6962,\n","      \"has_flex\": false\n","    }\n","  ],\n","  \"notes\": [\n","    \"All variants BUILTINS-only (Docker-safe).\"\n","  ]\n","}\n","\n","=== Converting: TinyFormer ===\n","Sources: {'savedmodel': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_TinyFormer_Model_48_12', 'keras': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_TinyFormer_Model_48_12.keras', 'h5': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_TinyFormer_Model_48_12.h5', 'weights': '/content/drive/MyDrive/EdgeMeter_AIv2/data/lstm_48_12_trained.weights.h5'}\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/TinyFormer/TinyFormer_fp32.tflite  (0.3034 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/TinyFormer/TinyFormer_fp16.tflite  (0.1609 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/TinyFormer/TinyFormer_int8_dynamic.tflite  (0.1019 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/TinyFormer/TinyFormer_int8_full.tflite  (0.1043 MB)\n","{\n","  \"model\": \"TinyFormer\",\n","  \"artifacts\": [\n","    {\n","      \"variant\": \"fp32\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/TinyFormer/TinyFormer_fp32.tflite\",\n","      \"size_mb\": 0.3034,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"fp16\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/TinyFormer/TinyFormer_fp16.tflite\",\n","      \"size_mb\": 0.1609,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"int8_dynamic\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/TinyFormer/TinyFormer_int8_dynamic.tflite\",\n","      \"size_mb\": 0.1019,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"int8_full\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/TinyFormer/TinyFormer_int8_full.tflite\",\n","      \"size_mb\": 0.1043,\n","      \"has_flex\": false\n","    }\n","  ],\n","  \"notes\": [\n","    \"All variants BUILTINS-only (Docker-safe).\"\n","  ]\n","}\n","\n","=== Converting: LiPFormer ===\n","Sources: {'savedmodel': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LiPFormer_Model_48_12', 'keras': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LiPFormer_Model_48_12.keras', 'h5': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LiPFormer_Model_48_12.h5', 'weights': '/content/drive/MyDrive/EdgeMeter_AIv2/data/lstm_48_12_trained.weights.h5'}\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiPFormer/LiPFormer_fp32.tflite  (0.4468 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiPFormer/LiPFormer_fp16.tflite  (0.2327 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiPFormer/LiPFormer_int8_dynamic.tflite  (0.1375 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiPFormer/LiPFormer_int8_full.tflite  (0.1426 MB)\n","{\n","  \"model\": \"LiPFormer\",\n","  \"artifacts\": [\n","    {\n","      \"variant\": \"fp32\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiPFormer/LiPFormer_fp32.tflite\",\n","      \"size_mb\": 0.4468,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"fp16\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiPFormer/LiPFormer_fp16.tflite\",\n","      \"size_mb\": 0.2327,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"int8_dynamic\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiPFormer/LiPFormer_int8_dynamic.tflite\",\n","      \"size_mb\": 0.1375,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"int8_full\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiPFormer/LiPFormer_int8_full.tflite\",\n","      \"size_mb\": 0.1426,\n","      \"has_flex\": false\n","    }\n","  ],\n","  \"notes\": [\n","    \"All variants BUILTINS-only (Docker-safe).\"\n","  ]\n","}\n","\n","=== Converting: LiteFormer ===\n","Sources: {'savedmodel': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LiteFormer_Model_48_12', 'keras': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LiteFormer_Model_48_12.keras', 'h5': '/content/drive/MyDrive/EdgeMeter_AIv2/data/Final_LiteFormer_Model_48_12.h5', 'weights': '/content/drive/MyDrive/EdgeMeter_AIv2/data/lstm_48_12_trained.weights.h5'}\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiteFormer/LiteFormer_fp32.tflite  (0.3511 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiteFormer/LiteFormer_fp16.tflite  (0.1836 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiteFormer/LiteFormer_int8_dynamic.tflite  (0.1093 MB)\n","  wrote /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiteFormer/LiteFormer_int8_full.tflite  (0.1176 MB)\n","{\n","  \"model\": \"LiteFormer\",\n","  \"artifacts\": [\n","    {\n","      \"variant\": \"fp32\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiteFormer/LiteFormer_fp32.tflite\",\n","      \"size_mb\": 0.3511,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"fp16\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiteFormer/LiteFormer_fp16.tflite\",\n","      \"size_mb\": 0.1836,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"int8_dynamic\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiteFormer/LiteFormer_int8_dynamic.tflite\",\n","      \"size_mb\": 0.1093,\n","      \"has_flex\": false\n","    },\n","    {\n","      \"variant\": \"int8_full\",\n","      \"path\": \"/content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/LiteFormer/LiteFormer_int8_full.tflite\",\n","      \"size_mb\": 0.1176,\n","      \"has_flex\": false\n","    }\n","  ],\n","  \"notes\": [\n","    \"All variants BUILTINS-only (Docker-safe).\"\n","  ]\n","}\n","\n","All requested models converted (Flex-free) or raised an actionable error.\n"]}],"source":["# === EdgeMeter V2 → Docker-safe TFLite exporter (TF 2.19 / Keras 3) ===\n","# Outputs per model: fp32, fp16, int8_dynamic, int8_full (float32 I/O), BUILTINS-only.\n","# LSTM is rebuilt as static+unrolled (batch=1) with dropouts=0.0 to avoid TensorList/Flex.\n","# Manifests are saved under: /content/drive/MyDrive/EdgeMeter_AIv2/data/tflite/{MODEL}/manifest.json\n","\n","import os, json, glob, io, contextlib\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# ----------------- Config -----------------\n","V2_DIR   = \"/content/drive/MyDrive/EdgeMeter_AIv2/data\"\n","X_TEST   = os.path.join(V2_DIR, \"X_test_48_12.npy\")\n","OUT_ROOT = os.path.join(V2_DIR, \"tflite\")\n","MODELS   = [\"LSTM\", \"TinyFormer\", \"LiPFormer\", \"LiteFormer\"]\n","\n","os.makedirs(OUT_ROOT, exist_ok=True)\n","print(\"TF:\", tf.__version__)\n","\n","# CPU-only for deterministic conversion\n","try:\n","    tf.config.experimental.set_visible_devices([], 'GPU')\n","    print(\"GPU hidden (CPU-only).\")\n","except Exception as e:\n","    print(\"Note:\", e)\n","\n","# ----------------- Helpers -----------------\n","def xfeat_dim(x_path: str) -> int:\n","    x = np.load(x_path, mmap_mode=\"r\")\n","    if x.ndim != 3 or x.shape[1] != 48:\n","        raise ValueError(f\"Expected (N,48,F) at {x_path}, got {x.shape}\")\n","    return int(x.shape[2])\n","\n","def find_sources(model_name: str):\n","    base = f\"Final_{model_name}_Model_48_12\"\n","    return {\n","        \"savedmodel\": os.path.join(V2_DIR, base) if os.path.isdir(os.path.join(V2_DIR, base)) else \"\",\n","        \"keras\":      os.path.join(V2_DIR, base + \".keras\") if os.path.isfile(os.path.join(V2_DIR, base + \".keras\")) else \"\",\n","        \"h5\":         os.path.join(V2_DIR, base + \".h5\") if os.path.isfile(os.path.join(V2_DIR, base + \".h5\")) else \"\",\n","        \"weights\":    next(iter(glob.glob(os.path.join(V2_DIR, \"**\", \"*weights*.h5\"), recursive=True)), \"\")\n","    }\n","\n","# --- Robust analyzer: capture stdout OR scan binary; never returns None ---\n","def analyze_tflite(path: str) -> dict:\n","    rep_text = \"\"\n","    try:\n","        buf = io.StringIO()\n","        with contextlib.redirect_stdout(buf):\n","            _ = tf.lite.experimental.Analyzer.analyze(model_path=path)\n","        rep_text = buf.getvalue() or \"\"\n","    except Exception as e:\n","        rep_text = f\"[analyzer exception] {e}\"\n","\n","    # Fallback: scan flatbuffer for tell-tale strings\n","    try:\n","        if not rep_text.strip():\n","            with open(path, \"rb\") as fh:\n","                blob = fh.read()\n","            rep_text = blob.decode(\"latin-1\", errors=\"ignore\")\n","    except Exception as e:\n","        rep_text += f\"\\n[binary scan skipped: {e}]\"\n","\n","    flags = [\"Flex\", \"SELECT_TF_OPS\", \"Unsupported ops\", \"Unsupported operation\"]\n","    has_flex = any(flag in rep_text for flag in flags)\n","    return {\"has_flex\": has_flex, \"report\": rep_text}\n","\n","def tflite_write(b: bytes, out_path: str) -> float:\n","    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n","    with open(out_path, \"wb\") as f: f.write(b)\n","    sz_mb = round(os.path.getsize(out_path) / (1024*1024), 4)\n","    print(f\"  wrote {out_path}  ({sz_mb} MB)\")\n","    return sz_mb\n","\n","def representative_dataset(x: np.ndarray, n=256):\n","    n = min(n, x.shape[0])\n","    for i in range(n):\n","        yield [x[i:i+1].astype(np.float32)]\n","\n","def mk_converter_from_savedmodel(path: str):\n","    conv = tf.lite.TFLiteConverter.from_saved_model(path)\n","    try: conv._experimental_lower_tensor_list_ops = True\n","    except: pass\n","    try: conv.experimental_enable_resource_variables = True\n","    except: pass\n","    conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n","    conv.allow_custom_ops = False\n","    return conv\n","\n","def mk_converter_from_keras(km: keras.Model):\n","    conv = tf.lite.TFLiteConverter.from_keras_model(km)\n","    try: conv._experimental_lower_tensor_list_ops = True\n","    except: pass\n","    try: conv.experimental_enable_resource_variables = True\n","    except: pass\n","    conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n","    conv.allow_custom_ops = False\n","    return conv\n","\n","# -------- LSTM export-friendly rebuild (static, unrolled) --------\n","def rebuild_lstm_static_unrolled(units: int, feat_dim: int) -> keras.Model:\n","    # Static batch=1, unroll=True, dropouts=0.0, implementation=1\n","    m = keras.Sequential([\n","        keras.layers.Input(batch_shape=(1, 48, feat_dim)),\n","        keras.layers.LSTM(\n","            units,\n","            activation=\"tanh\",\n","            recurrent_activation=\"sigmoid\",\n","            dropout=0.0,\n","            recurrent_dropout=0.0,\n","            unroll=True,\n","            implementation=1,\n","            stateful=False,\n","            return_sequences=False\n","        ),\n","        keras.layers.Dense(12)\n","    ], name=\"LSTM_export_static_unrolled\")\n","    return m\n","\n","def try_load_keras_model(path: str):\n","    if not path: return None\n","    return keras.models.load_model(path, compile=False, safe_mode=False)\n","\n","# -------- Variant conversion wrappers --------\n","def convert_variants_from_savedmodel(src_sm_dir: str, out_dir: str, label: str):\n","    x = np.load(X_TEST, mmap_mode=\"r\")\n","    manifest = {\"model\": label, \"artifacts\": [], \"notes\": []}\n","\n","    # fp32\n","    c = mk_converter_from_savedmodel(src_sm_dir)\n","    b = c.convert()\n","    p = os.path.join(out_dir, f\"{label}_fp32.tflite\"); s = tflite_write(b, p)\n","    a = analyze_tflite(p); manifest[\"artifacts\"].append({\"variant\":\"fp32\",\"path\":p,\"size_mb\":s,\"has_flex\":a[\"has_flex\"]})\n","\n","    # fp16\n","    c = mk_converter_from_savedmodel(src_sm_dir); c.optimizations=[tf.lite.Optimize.DEFAULT]; c.target_spec.supported_types=[tf.float16]\n","    b = c.convert()\n","    p = os.path.join(out_dir, f\"{label}_fp16.tflite\"); s = tflite_write(b, p)\n","    a = analyze_tflite(p); manifest[\"artifacts\"].append({\"variant\":\"fp16\",\"path\":p,\"size_mb\":s,\"has_flex\":a[\"has_flex\"]})\n","\n","    # int8 dynamic\n","    c = mk_converter_from_savedmodel(src_sm_dir); c.optimizations=[tf.lite.Optimize.DEFAULT]\n","    b = c.convert()\n","    p = os.path.join(out_dir, f\"{label}_int8_dynamic.tflite\"); s = tflite_write(b, p)\n","    a = analyze_tflite(p); manifest[\"artifacts\"].append({\"variant\":\"int8_dynamic\",\"path\":p,\"size_mb\":s,\"has_flex\":a[\"has_flex\"]})\n","\n","    # int8 full (float32 IO)\n","    c = mk_converter_from_savedmodel(src_sm_dir)\n","    c.optimizations=[tf.lite.Optimize.DEFAULT]\n","    c.representative_dataset = lambda: representative_dataset(x, 256)\n","    c.inference_input_type  = tf.float32\n","    c.inference_output_type = tf.float32\n","    b = c.convert()\n","    p = os.path.join(out_dir, f\"{label}_int8_full.tflite\"); s = tflite_write(b, p)\n","    a = analyze_tflite(p); manifest[\"artifacts\"].append({\"variant\":\"int8_full\",\"path\":p,\"size_mb\":s,\"has_flex\":a[\"has_flex\"]})\n","\n","    offenders = [a[\"variant\"] for a in manifest[\"artifacts\"] if a[\"has_flex\"]]\n","    manifest[\"notes\"].append(\"All variants BUILTINS-only (Docker-safe).\" if not offenders\n","                             else f\"WARNING: Flex required for {offenders} → not Docker-safe.\")\n","    with open(os.path.join(out_dir, \"manifest.json\"), \"w\") as f: json.dump(manifest, f, indent=2)\n","    print(json.dumps(manifest, indent=2))\n","    return manifest\n","\n","def convert_variants_from_keras(km: keras.Model, out_dir: str, label: str):\n","    x = np.load(X_TEST, mmap_mode=\"r\")\n","    manifest = {\"model\": label, \"artifacts\": [], \"notes\": []}\n","\n","    # fp32\n","    c = mk_converter_from_keras(km); b = c.convert()\n","    p = os.path.join(out_dir, f\"{label}_fp32.tflite\"); s = tflite_write(b, p)\n","    a = analyze_tflite(p); manifest[\"artifacts\"].append({\"variant\":\"fp32\",\"path\":p,\"size_mb\":s,\"has_flex\":a[\"has_flex\"]})\n","\n","    # fp16\n","    c = mk_converter_from_keras(km); c.optimizations=[tf.lite.Optimize.DEFAULT]; c.target_spec.supported_types=[tf.float16]\n","    b = c.convert()\n","    p = os.path.join(out_dir, f\"{label}_fp16.tflite\"); s = tflite_write(b, p)\n","    a = analyze_tflite(p); manifest[\"artifacts\"].append({\"variant\":\"fp16\",\"path\":p,\"size_mb\":s,\"has_flex\":a[\"has_flex\"]})\n","\n","    # int8 dynamic\n","    c = mk_converter_from_keras(km); c.optimizations=[tf.lite.Optimize.DEFAULT]\n","    b = c.convert()\n","    p = os.path.join(out_dir, f\"{label}_int8_dynamic.tflite\"); s = tflite_write(b, p)\n","    a = analyze_tflite(p); manifest[\"artifacts\"].append({\"variant\":\"int8_dynamic\",\"path\":p,\"size_mb\":s,\"has_flex\":a[\"has_flex\"]})\n","\n","    # int8 full (float32 IO)\n","    c = mk_converter_from_keras(km); c.optimizations=[tf.lite.Optimize.DEFAULT]\n","    c.representative_dataset = lambda: representative_dataset(x, 256)\n","    c.inference_input_type  = tf.float32\n","    c.inference_output_type = tf.float32\n","    b = c.convert()\n","    p = os.path.join(out_dir, f\"{label}_int8_full.tflite\"); s = tflite_write(b, p)\n","    a = analyze_tflite(p); manifest[\"artifacts\"].append({\"variant\":\"int8_full\",\"path\":p,\"size_mb\":s,\"has_flex\":a[\"has_flex\"]})\n","\n","    offenders = [a[\"variant\"] for a in manifest[\"artifacts\"] if a[\"has_flex\"]]\n","    manifest[\"notes\"].append(\"All variants BUILTINS-only (Docker-safe).\" if not offenders\n","                             else f\"WARNING: Flex required for {offenders} → not Docker-safe.\")\n","    with open(os.path.join(out_dir, \"manifest.json\"), \"w\") as f: json.dump(manifest, f, indent=2)\n","    print(json.dumps(manifest, indent=2))\n","    return manifest\n","\n","# ----------------- Per-model conversion -----------------\n","def convert_model(model_name: str):\n","    print(f\"\\n=== Converting: {model_name} ===\")\n","    src = find_sources(model_name)\n","    print(\"Sources:\", src)\n","    out_dir = os.path.join(OUT_ROOT, model_name)\n","    os.makedirs(out_dir, exist_ok=True)\n","\n","    if model_name == \"LSTM\":\n","        # Prefer .keras / .h5 to rebuild static+unrolled graph and transfer weights.\n","        km_base = None\n","        if src[\"keras\"]:\n","            km_base = try_load_keras_model(src[\"keras\"])\n","        elif src[\"h5\"]:\n","            km_base = try_load_keras_model(src[\"h5\"])\n","\n","        if km_base is not None:\n","            feat_dim = xfeat_dim(X_TEST)\n","            lstm_layers = [l for l in km_base.layers if isinstance(l, keras.layers.LSTM)]\n","            if not lstm_layers:\n","                raise RuntimeError(\"No LSTM layer found in LSTM model file.\")\n","            units = lstm_layers[0].units\n","\n","            clean = rebuild_lstm_static_unrolled(units, feat_dim)\n","            # Weight transfer: assumes architecture Input->LSTM->(optional Dropout)->Dense(12)\n","            clean.set_weights(km_base.get_weights())\n","\n","            manifest = convert_variants_from_keras(clean, out_dir, label=model_name)\n","\n","        elif src[\"savedmodel\"]:\n","            # Try direct from SavedModel; if Flex appears, abort with instructions.\n","            manifest = convert_variants_from_savedmodel(src[\"savedmodel\"], out_dir, label=model_name)\n","            offenders = [a[\"variant\"] for a in manifest[\"artifacts\"] if a[\"has_flex\"]]\n","            if offenders:\n","                raise RuntimeError(\n","                    \"LSTM SavedModel conversion requires Flex (likely due to dropout/recurrent_dropout>0). \"\n","                    \"Provide a .keras/.h5 (or weights) so we can rebuild an export-friendly LSTM \"\n","                    \"(dropouts=0.0, static batch=1, unrolled) and reconvert.\"\n","                )\n","        else:\n","            raise FileNotFoundError(\"No LSTM artifacts found (.keras/.h5/SavedModel).\")\n","\n","    else:\n","        # Transformers: prefer SavedModel, else .keras/.h5\n","        if src[\"savedmodel\"]:\n","            manifest = convert_variants_from_savedmodel(src[\"savedmodel\"], out_dir, label=model_name)\n","        elif src[\"keras\"] or src[\"h5\"]:\n","            km = try_load_keras_model(src[\"keras\"] or src[\"h5\"])\n","            manifest = convert_variants_from_keras(km, out_dir, label=model_name)\n","        else:\n","            raise FileNotFoundError(f\"No artifacts found for {model_name}.\")\n","\n","        offenders = [a[\"variant\"] for a in manifest[\"artifacts\"] if a[\"has_flex\"]]\n","        if offenders:\n","            raise RuntimeError(\n","                f\"{model_name} conversion introduced Flex in {offenders}. \"\n","                \"Check for unsupported activations/layers and keep to TFLite builtins.\"\n","            )\n","\n","# ----------------- Main -----------------\n","def main():\n","    _ = xfeat_dim(X_TEST)  # sanity check\n","    for m in MODELS:\n","        convert_model(m)\n","    print(\"\\nAll requested models converted (Flex-free) or raised an actionable error.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}